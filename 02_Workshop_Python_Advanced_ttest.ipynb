{"cells":[{"cell_type":"markdown","metadata":{"id":"JVr38wdCkA-f"},"source":["# Hypothesis Tests\n","\n","Sometimes we want to know if a difference between two groups is real and meaningful, not just a random fluke. To figure this out, we use something called a hypothesis test.\n","\n","A test like this helps us see how likely it is that our results are due to a real effect. We talk about this using terms like **statistical significance** and **effect size**.\n","\n","As always, everything starts with data!"]},{"cell_type":"markdown","metadata":{"id":"-UfHuBPtlGF7"},"source":["## Loading and Understanding the Data\n","<font color='green'>**Let's load the data (`\"02_Sales.csv\"`) and get to know it a bit.**</font>\n","\n","The data is synthetically generated for practicing statistical methods and is based on this [Kaggle Link](https://www.kaggle.com/datasets/matinmahmoudi/sales-and-satisfaction/data?select=Sales_without_NaNs_v1.3.csv).\n","\n","If the below code does not work for you, try running either of the following lines first:\n","- `!pip install matplotlib`\n","- `%pip install matplotlib`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aISftbMBj1FM"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","df = pd.read_csv(\"02_Sales.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FI3kA46o7Kcc"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"g7CHhWnZ9l87"},"source":["<font color='green'>**What kind of analyses could we perform with this dataset? Take a moment to think of a few ideas before moving on.**</font>"]},{"cell_type":"markdown","metadata":{"id":"Y0lQcmRb99KF"},"source":["# Possible Analyses\n","## Correlations\n","- How much is customer satisfaction related to sales? → **Pearson's or Spearman's Correlation**\n","- How does the satisfaction or sales *before* relate to the satisfaction or sales *after*? → **Pearson's or Spearman's Correlation**\n","\n","## Group Differences\n","- Are the sales or satisfaction of the Treatment group significantly different from the Control group? → **Independent Samples t-test**\n","- Did sales or satisfaction significantly change for a single group when you compare *before* and *after*? → **Paired Samples t-test**\n","- Did the Treatment group make significantly more purchases than the Control group? → **Bonus: Chi-Squared Test**"]},{"cell_type":"markdown","metadata":{"id":"zVtPydhwBXBY"},"source":["# Independent Samples t-test\n","## _When_ do we use it?\n","An **independent samples t-test** helps us determine how likely it is that an observed difference **between two separate groups** happened just by random chance. This likelihood is represented by a **p-value**.\n","\n","If the p-value is very small (a common threshold is $p < 0.05$), we say the result is **statistically significant**. This means there's less than a 5% probability that we would see such a difference if there were no real effect. In other words, we can be about 95% confident that the difference between the groups is real and is linked to what makes the groups different (e.g., Medicine vs. Placebo, Website Design A vs. B, etc.).\n","\n","## _How_ do we use it?\n","\n","It's quite simple in Python! We just need to identify our grouping variable and our target variable.\n","\n","In our case, we'll use `Group` (Control vs. Treatment) as our grouping variable and `Sales_After` as our numerical target variable.\n","\n","**A quick note:**\n","- We could (and probably should) also check the `Sales_Before` variable. Why? To make sure there wasn't already a significant difference between the groups *before* our intervention. If the Treatment group was already full of high-spending customers by chance, our results would be biased.\n","- We could do the exact same analysis for the `Satisfaction` variable.\n","- The `Purchase_Made` variable is categorical (yes/no), not numerical, so it's **not** suitable for a t-test."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"baXqnV33lvZw"},"outputs":[],"source":["from scipy import stats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TGL8GBfwmfx2"},"outputs":[],"source":["# We filter our DataFrame for each group and then select our target variable.\n","# A t-test is symmetrical, so the order of the groups doesn't matter.\n","\n","t_stat, p_value = stats.ttest_ind(\n","    df[df.Group == \"Control\"].Sales_After,\n","    df[df.Group == \"Treatment\"].Sales_After\n","    )\n","\n","print(f\"p-value: {p_value}\")"]},{"cell_type":"markdown","metadata":{"id":"xrpFDGeRHYL-"},"source":["In this example, the p-value is extremely small (it's displayed as 0.0), which is much less than our 0.05 threshold. Therefore, we can conclude that the intervention that separated the Treatment group from the Control group had a statistically significant effect on sales.\n","\n","But don't celebrate just yet! A t-test, much like Pearson's correlation, has a few assumptions we should check to make sure our result is trustworthy."]},{"cell_type":"markdown","metadata":{"id":"AA7hURnnH_XQ"},"source":["## Assumptions of an Independent Samples t-test\n","A t-test is only truly reliable if the underlying data meets certain conditions:\n","\n","- 1) The sample should be representative of the population we want to talk about: (✅) → We'll assume this is true for our data.\n","\n","- 2) The target variable must be numerical: ✅\n","\n","- 3) The observations should be independent: (✅) → We'll assume this is true.\n","\n","- 4) No significant outliers: ❓ → We need to check this!\n","\n","- 5) The target variable should be normally distributed within each group: ❓ → We need to check this!\n","\n","- 6) Homogeneity of variances (the spread of the data in both groups should be similar): ❓ → We need to check this!"]},{"cell_type":"markdown","metadata":{"id":"B8PgUfS2-Q5o"},"source":["### 4. Checking for Outliers"]},{"cell_type":"markdown","metadata":{"id":"z6N7-4t5_jiE"},"source":["We can use a boxplot to visualize outliers. It looks like we might have a few high and low outliers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6FPr_Ms0-3Xy"},"outputs":[],"source":["df.boxplot(column=\"Sales_After\", by=\"Group\")\n","plt.title(\"Sales After Intervention by Group\")\n","plt.suptitle('') # Suppress the automatic title\n","plt.xlabel(\"Group\")\n","plt.ylabel(\"Sales After\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"JnJFNcoiAjuL"},"source":["However, if we look at these data points in the context of their spending *before* the intervention, these 'outliers' start to make sense. They seem to be customers who simply spend a lot (or a little) in general. So, it's not surprising that their spending is also higher or lower than the majority after the intervention.\n","\n","For now, I see no strong reason to remove them. We could investigate the most extreme points individually if we wanted to be extra thorough."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E6rYZSz9-nOo"},"outputs":[],"source":["color_map = {\"Control\": \"grey\", \"Treatment\": \"red\"}\n","colors = df[\"Group\"].map(color_map)\n","\n","df.plot.scatter(x=\"Sales_Before\", y=\"Sales_After\", c=colors)\n","plt.title(\"Spending Before vs. After\")\n","plt.xlabel(\"Sales Before\")\n","plt.ylabel(\"Sales After\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"TzVd21vt7j93"},"source":["### 5. Checking for Normality"]},{"cell_type":"markdown","metadata":{"id":"pVXlR9Ow88lY"},"source":["Just by looking at a histogram, both groups seem to be roughly bell-shaped (i.e., normally distributed). Since the t-test is pretty robust against slight deviations from normality, especially with larger datasets, this visual check is often good enough to start."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hrzcdbwo8umN"},"outputs":[],"source":["df[df.Group == \"Control\"].Sales_After.hist(alpha=0.7, label='Control')\n","df[df.Group == \"Treatment\"].Sales_After.hist(alpha=0.7, label='Treatment')\n","plt.legend()\n","plt.title(\"Distribution of Sales After\")\n","plt.xlabel(\"Sales\")\n","plt.ylabel(\"Frequency\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"4-3FmpBI0dlO"},"source":["#### Statistical Tests for Normality\n","Normality can also be tested statistically, using the **Shapiro-Wilk Test** and the **Kolmogorov-Smirnov (KS)** Test, both named after their creators.\n","\n","- **Shapiro-Wilk** is particularly suitable for small samples (fewer than 2000 data points, and even under 50).\n","- **KS** requires larger data sets of at least 50 to yield meaningful results."]},{"cell_type":"markdown","metadata":{"id":"9rcQa8xf0dlP"},"source":["Let's first count how many data points we have to test:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5kAD9aV0dlP"},"outputs":[],"source":["print(df[df.Group == \"Control\"].Sales_After.shape[0])\n","print(df[df.Group == \"Treatment\"].Sales_After.shape[0])"]},{"cell_type":"markdown","metadata":{"id":"WrwOodPk0dlP"},"source":["We have around 5,000 data points in each group. Therefore, the KS test for normality is probably the more appropriate choice.\n","\n","But I’ll show you both nonetheless:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nr3SdJWA0dlP"},"outputs":[],"source":["from scipy.stats import shapiro\n","\n","# Perform Shapiro-Wilk test\n","stat, p_value = shapiro(df[df.Group == \"Control\"].Sales_After)\n","\n","print(f\"Shapiro-Wilk Test Statistic: {stat}\")\n","print(f\"P-Value: {p_value}\")\n","\n","# Interpret result\n","if p_value > 0.05:\n","    print(\"Data is likely normal\")\n","else:\n","    print(\"Data is not normal\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"McP1REVp0dlP"},"outputs":[],"source":["from scipy.stats import kstest\n","\n","# Perform Kolmogorov-Smirnov test for normality\n","stat, p_value = kstest(df[df.Group == \"Control\"].Sales_After, 'norm')\n","\n","print(f\"Kolmogorov-Smirnov Test Statistic: {stat}\")\n","print(f\"P-Value: {p_value}\")\n","\n","# Interpret result\n","if p_value > 0.05:\n","    print(\"Data is likely normal\")\n","else:\n","    print(\"Data is not normal (reject H0)\")\n"]},{"cell_type":"markdown","metadata":{"id":"LV2Tlp8T0dlP"},"source":["<font color='green'>**Try running the normality test for the other group we want to examine.**</font>"]},{"cell_type":"code","source":["# Your code here"],"metadata":{"id":"qCIFMQB507M6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6uhfooao0dlP"},"source":["So... Strictly speaking the normality assumption is violated (the data is not normally distributed). But the t-test is somewhat robust against such violations and judging visually, the distribution does not look too far off... Let's proceed."]},{"cell_type":"markdown","metadata":{"id":"wdn95gaSCO-R"},"source":["### 6. Checking for Homogeneity of Variances\n","An independent samples t-test works best when the variances (the 'spread' of the data) of the two groups are roughly equal.\n","\n","Our boxplot from earlier gave us a hint, but we can test this assumption more formally with a **Levene test**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SLfaW-hWClgo"},"outputs":[],"source":["from scipy.stats import levene\n","\n","stat, p = levene(df[df.Group == \"Control\"].Sales_After, df[df.Group == \"Treatment\"].Sales_After)\n","print(f\"Levene’s test statistic: {stat:.4f}, p-value: {p:.4f}\")\n","\n","# Interpretation\n","if p > 0.05:\n","    print(\"The variances are likely equal (p > 0.05)\")\n","else:\n","    print(\"The variances are likely not equal (p <= 0.05)\")"]},{"cell_type":"markdown","metadata":{"id":"pqCHbacEDux9"},"source":["### A Quick Correction!\n","\n","The Levene test result tells us that the variances of our two groups are **not equal**. This violates one of the assumptions of the standard t-test.\n","\n","Luckily, there's an easy fix! We can use a variation of the t-test, called **Welch's t-test**, which does not assume equal variances. It's the correct and more robust choice here.\n","\n","To do this in Python, we simply run the same function as before, but add the parameter `equal_var=False`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"welch_test_cell"},"outputs":[],"source":["# Rerunning the t-test with Welch's correction for unequal variances\n","t_stat_welch, p_value_welch = stats.ttest_ind(\n","    df[df.Group == \"Control\"].Sales_After,\n","    df[df.Group == \"Treatment\"].Sales_After,\n","    equal_var=False # This is the important part\n","    )\n","\n","print(f\"Welch's t-test p-value: {p_value_welch}\")\n","\n","if p_value_welch < 0.05:\n","  print(\"\\nOur conclusion remains the same: there is a significant difference between the groups!\")\n","else:\n","  print(f\"Welch's test shows there is no significant difference between the groups (p={p_value_welch})\")"]},{"cell_type":"markdown","metadata":{"id":"3l3mYF2tD5jh"},"source":["# Your Turn!\n","<font color='green'>**Now, try to follow the same steps to test for a difference between the groups in `Customer_Satisfaction_After`.**</font>\n","\n","1.  Run a t-test on `Customer_Satisfaction_After`.\n","2.  Check the assumptions (especially normality and homogeneity of variances).\n","3.  Decide if you need to use Welch's t-test.\n","4.  State your final conclusion!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nsZOc_708zuW"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"bJZLJWRD0dlQ"},"source":["# Bonus: Paired Samples T-Test\n","So far, we analysed differences between two **independent groups**. However, often we may perform two measurements of the **same group at two points in time**. This is called a _paired sample_.\n","The assumptions, execution and interpretation are similar to the independent samples t-test.\n","\n","<font color='green'>**If you're interested and have the time, try to run a paired samples t-test for two separate columns on the same group.**</font>\n","\n","For the sake of brevity, we'll avoid testing all assumptions, here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2PW1_x-L0dlQ"},"outputs":[],"source":["from scipy.stats import ttest_rel\n","\n","# ttest_rel(..., ...)"]},{"cell_type":"markdown","metadata":{"id":"UqJGIIDYELXx"},"source":["# Bonus: Chi-Squared Test\n","Want to dive a little deeper into statistics? Try using an AI assistant or other online resources to learn about the **Chi-Squared Test**.\n","\n","You can use it to test whether the intervention was successful in getting more customers to make a purchase (`Purchase_Made`).\n","\n","**Hint:** We need a Chi-Squared test here because our target variable (`Purchase_Made`) is categorical (True/False), not numerical."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PZowNQWG0dlQ"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}